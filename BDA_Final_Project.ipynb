{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CX0ZDyhSi8-3",
    "tags": []
   },
   "source": [
    "# BDA Final Project\n",
    "# Spotify Podcasts Analysis\n",
    "\n",
    "Xavier Cucurull Salamero\n",
    "\n",
    "December 2021\n",
    "\n",
    "<center><img src='Docs/img/spotify_bda_logo.png'  width=\"500\" ></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqAImhsNjKZk"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Podcasts are an audio-only mean of communication that is rapidly growing. Spotify, one of the major audio streaming services has recently put more attention into podcasting. Last month the company expanded their Podcast Subscription program to global markets [1] and in their Q3 shareholder letter the company [2] pointed out that podcasts on Spotify are up to 3.2M from 2.9M the previous quarter, maintaining also a strong monthly active users engagement.\n",
    "\n",
    "With the growth of podcasting, getting insights on the market evolution is crucial for many stakeholders. Podchaser, the \"IMDb for podcasts\", which provides payment services targeted at marketers, PR firms, and other professionals to get relevant insights and metrics, announced in January $4M in funding [3].\n",
    "\n",
    "I am a podcast enthusiast and I have recently found myself consuming content that is only available online, as opposed to those podcasts that are also broadcast live on the radio, with some podcasts being Spotify exclusive. In addition, during the COVID-19 lockdown I had the impression that new podcasts were created every week, as if almost everybody wanted to make one. As a result, I thought that conducting an analysis on the podcast market could be really interesting and a good challenge to apply the knowledgee obtained during the course.\n",
    "\n",
    "This project is divided in two parts. First, the development of a podcast scraper using the Spotify API [4] with the goal of constructing a database containing various information about the show such as name, publisher and release date. Second, a big data analysis on the obtained dataset, with the goal of getting insight on the evolution and current state of the podcast market. For simplicity, given the long processing time necessary to construct the database, the scope of the study has been narrowed down to podcasts in Catalan and Spanish, which are my two native languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smt3fRXIjcqY",
    "tags": []
   },
   "source": [
    "## 2. Data collection\n",
    "Being a Spotify user, I first searched for their API to see what could be done and I saw that last year Spotify introduced the podcasts API [5]. This seemed promising because compared to other services such as Podchaser, which has a limit of 25,000 monthly/requests on their free plan, Spotify desn't have any restrictions in terms of the total number of queries, only a quota related to the amount of requests that can be made in a 30 seconds window.\n",
    "\n",
    "The main Spotify web API function is \"Search\", which allows to get catalog information about albums, artists, playlists, tracks, shows or episodes that match a keyword string. So in order to get a list of all the podcasts in the Spotify catalog it was necessary to find those keywords that would retrieve them. With the API limit of 1000 responses for request this seemed like an impossible task. How could I think of those words or strings that would lead me to discover the titles of all the podcasts available? Before using the Spotify API to get information about the shows I needed to find a list of podcast names. To do that I thought of another podcast provider, Apple's iTunes [6]. Being part of the Apple Developer Program costs $99 [7], which is not affordable for the scope of this project. Since I just needed to get a list of the names of all the podcasts available on the iTunes catalog, I could write a simple Python script to perform the web scraping. Once I got this list, I could search for each of the names using the Spotify API. Below there is a diagram of the database creation process.\n",
    "\n",
    "<center><figure>\n",
    "    <img src='Docs/img/database_creation_diagram.png' width=\"450\">\n",
    "    <figcaption><i>Fig 1. Database creation diagram<i><figcaption>\n",
    "<figure></center>\n",
    "        \n",
    "\n",
    "### 2.1. Apple Podcasts scraper\n",
    "The first part of the database construction is the Apple Podcasts scraper. This Python script uses the ```requests``` and ```beautifulsoup``` packages to parse the html files of the Apple Podcasts page, exploring all the different categories, and creates a CSV table with the title and genre of all the podcasts in the catalog.\n",
    "\n",
    "Since the podcast shows are sorted by genre, the first step is to get all the genres and their corresponding url. Figure 2 shows the rendred website and the html code of the iTunes Podcast genres page.\n",
    "\n",
    "<center><figure>\n",
    "    <img src='Docs/img/apple_podcasts_website.png' width=\"700\">\n",
    "    <figcaption><i>Fig 2. iTunes Podcasts website - Genres<i><figcaption>\n",
    "<figure></center>   \n",
    "\n",
    "        \n",
    "Next, for each of the genres, the script iterates over all the pages that exist for each of the letters, as podcasts pages are organized and sorted alphabetically. Figure 3 shows the podcasts page for the genre \"Arte\".\n",
    "        \n",
    "<center><figure>\n",
    "    <img src='Docs/img/apple_podcasts_selectedgenre.png' width=\"700\">\n",
    "    <figcaption><i>Fig 3. iTunes Podcasts website - Shows<i><figcaption>\n",
    "<figure></center> \n",
    "\n",
    "The following code snippet, adapted from the ```get_podcasts_from_page``` function from the ```apple_podcats_scraper.py``` script, shows how a list of podcast titles can be obtained given the genre url, an alphabet letter and a page.\n",
    "\n",
    "        \n",
    "```python\n",
    "url = f'{genre_url}?letter={letter}&page={page}'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "selected_genre = soup.find(id='selectedgenre')\n",
    "podcasts = selected_genre.find(id='selectedcontent').find_all('li')\n",
    "\n",
    "podcasts_list.extend([p.get_text().rstrip() for p in podcasts])\n",
    "\n",
    "```\n",
    "\n",
    "Scraping content from the Apple iTunes Podcast webpage is a relatively simple process, since not a very high number of requests need to be made. As a result, the whole process was executed in 52 minutes, obtaining a final list of 2,056,848 podcasts. Below there's an extract of the debug messages printed by the ```apple_podcats_scraper.py``` script.\n",
    "        \n",
    "```\n",
    "[12:36:45] Starting podcast scraping...\n",
    "[12:36:45] Retrieving \"Arte\" podcasts...\n",
    "\tLetter A: 15360 podcasts (64 pages)\n",
    "\tLetter B: 11760 podcasts (49 pages)\n",
    "\tLetter C: 15600 podcasts (65 pages)\n",
    "\tLetter D: 10320 podcasts (43 pages)\n",
    "\tLetter E: 7440 podcasts (31 pages)\n",
    "\tLetter F: 7440 podcasts (31 pages)\n",
    "\tLetter G: 5760 podcasts (24 pages)\n",
    "\tLetter H: 7440 podcasts (31 pages)\n",
    "\tLetter I: 6000 podcasts (25 pages)\n",
    "```\n",
    "\n",
    "###Â 2.2. Spotify Podcasts scraper\n",
    "        \n",
    "The second and most important part of the database construction is the Spotify Podcast scraper. Using the list of Apple podcasts and Spotipy, which is a Python library for the Spotify Web API, this script iterates over each of the titles on the previously obtained podcast list and uses the search function to get those shows that match the provided string. Since some titles availabe in the Apple podcast list might not be present in the Spotify catalog and vice-versa, for each given title a maximum (configurable) number of 10 shows are retrieved. This allows for more \"exploration\" of the Spotify catalog, being able to discover shows that were not present in the original podcasts list. When searching for a show, the Spotify API returns information about the **id**, the **name**, the **description**, the **publisher**, the **total number of episodes**, whether it is flagged as **explicit**, the **languages** and **media type**. In order to obtain information about the release date and the date of the last show, the next step is to retrieve the episodes corresponding to each of the show. Since getting episode information adds a lot of overhead to the database creation process, at this step shows with languages Catalan (\"ca\") or Spanish (\"es) are filtered. In addition, in order to reduce the number of API calls, only a maximum number of 100 episodes (containing the first and the last) are retrieved. With those episodes, we obtain the **release date** of the show, the **last date** of the newest episode and the **average duration** of the episodes. Finally, the show is added to the database. Since the show id is used as a unique id in the database, the addition of duplicate entries is avoided.\n",
    "\n",
    "The process of scraping the Spotify catalog to create the podcasts database has been remarkably more complicated than obtaining the Apple podcast list. In order to avoid timeout errors, the Spotipy client has been configured with a high request timeout of 60 and the number of retries have also been increased to 10. In addition to that, a wait-retry strategy has also been implemented in order to overcome bad responses.\n",
    "\n",
    "The Apple podcasts list (```data/apple_podcasts.csv```) has a size of 76MB, so loading all its content in memory is perfecly feasible. However, in order to replicate a real-world scenario where the file to read is too big to read in memory, it is read in batches. In addition, in order to speed up the scraping process, the shows are retrieved in parallel using Python's ```multiprocessing```. Podcasts title are read in batches of 1000 and then processed. This batch approach also allows to stop the scraping process and start it later on, by indicating an *offset* so that a number of batches are skipped. This proved to be very useful since the process was sometimes interrupted due to connection problems or because the computer froze. \n",
    "        \n",
    "The ```SpotifyScraper``` class has been implemented in ```spotify_podcasts_scraper.py``` and the script that retrieves the shows and episode information and constructs the database is ```build_database.py```. Below there's an extract of the debug messages printed by the ```build_database.py``` script.\n",
    "        \n",
    "```\n",
    "[14:03:50] Processing batch 1478. 579 remaining...\n",
    "[14:05:48] 1034 shows retrieved from batch of 1000\n",
    "\n",
    "[14:05:48] Processing batch 1479. 578 remaining...\n",
    "Spotipy search error! Retrying in 30s...\n",
    "[14:09:40] 690 shows retrieved from batch of 1000\n",
    "\n",
    "```\n",
    "The number of shows retrieved correspond to those shows that are in Spanish or Catalan. Notice that it is possible than more than 1000 shows are retrieved from the batch because of the \"exploration\" mentioned above.\n",
    "\n",
    "T\n",
    "After the whole building process, which has taken 3 days and 23:18:52, a database with 263,202\n",
    "        \n",
    " \n",
    "### 2.3. MongoDB database\n",
    "Given the relative simplicity of the database to create and because of the fact that only one database -the podcasts database- is needed, MongoDB has been chosen for the project. \n",
    "\n",
    "The same docker container as the one used in Lab03 has been used. It is created by opening a terminal and running the following command:\n",
    "```docker run --name bda-mongo -e MONGO_INITDB_ROOT_USERNAME=mongoadmin -e MONGO_INITDB_ROOT_PASSWORD=pass1234 -d -p 27017:27017 mongo```\n",
    "        \n",
    "Once the container is running, the Python script connects to it and creates a database called ```finalproject``` and a collection with the name ```podcasts```, where the podcasts are stored. As learned in Lab03, Compass can be used to access the database using a graphical user interface. In addition, by connecting to the computer's IP and port it can even be accessed from another computer. In my case, I used a desktop computer running Ubuntu 20.04 to construct the database but I also monitored it using Compass on a laptop runing macOS.\n",
    "\n",
    "<center><figure>\n",
    "    <img src='Docs/img/compass.png' width=\"700\">\n",
    "    <figcaption><i>Fig 4. MongoDB Compass<i><figcaption>\n",
    "<figure></center> \n",
    "\n",
    "\n",
    "### 2.4. Limitations\n",
    "This method, although effective to build a big dataset that is representative of the podcasts market, has some limitations. First of all, although most podcasters try to have their show in platforms such as iTunes or Spotify, that is not always the case. In addition, with Spotify's increasing interest in podcasting, there are nowadays shows that are exclusive to Spotify. The titles of these shows don't appear on the Apple Podcasts list and, as a result, might be missed when scraping Spotify. One example is the podcast \"Oye Polo\", from Radio Primavera Sound, which was not captured by the database constructing method. \n",
    "\n",
    "### 2.5. Code Documentation\n",
    "All the Python scripts developed in this project have been documented using Docstrings and the corresponding documentation has been generated using Sphinx. As a reference, the documentation of all the code related to the database construction process can be found in the ```Docs/build/html``` directory or [here](Docs/build/html/py-modindex.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PulRLCZGjd-w"
   },
   "source": [
    "## 3. Data description\n",
    "In this section I will present the different fields of the documents of the database. I will use as an illustrative example the show \"Gent de Merda\", as shown in Compass.\n",
    "\n",
    "<center><figure>\n",
    "    <img src='Docs/img/compass_gentdemerda.png' width=\"550\">\n",
    "    <figcaption><i>Fig 5. Document example in Compass<i><figcaption>\n",
    "<figure></center> \n",
    "        \n",
    "Fields description:\n",
    "        \n",
    "- **_id**: MongoDB document identifier (ObjectId)\n",
    "        \n",
    "- **name**: Podcast name (string)\n",
    "        \n",
    "- **publisher**: Podcast publisher (string)\n",
    "        \n",
    "- **explicit**: Explicit content (boolean)\n",
    "        \n",
    "- **media_type**: Type of content (string)\n",
    "        \n",
    "- **id**: Spotify show ID (string)\n",
    "        \n",
    "- **languages**: A list of the languages used in the show, identified by their ISO 639 code (list of strings)\n",
    "        \n",
    "- **description**: Podcast description (string)\n",
    "        \n",
    "- **total_episodes**: Total number of episodes (int)\n",
    "        \n",
    "- **average_duration_min**: Average episode duration in minutes (int)\n",
    "        \n",
    "- **release_date**: Publishing date of the first episode\n",
    "    - *year*: (int)\n",
    "    - *month*: (int)\n",
    "    - *day*: (int)\n",
    "        \n",
    "- **last_date**: Publishing date of the last episode\n",
    "    - *year*: (int)\n",
    "    - *month*: (int)\n",
    "    - *day*: (int)\n",
    "        \n",
    "- **date_precision**: The precision with which the dates are known (string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data preprocessing\n",
    "\n",
    "Data preprocessing is an important step of the whole big data analysis process. It is important to identify and clean possible errors regarding missing values, outliers, etc. In our case, since all the data has been obtained using the Spotify API, it is not very likely that we will find a lot of noise or errors in the data. However, since the goal of the project is to study the podcasts market, it might be important to identify those shows which don't provide any significant information. For example, that would be podcasts without any episodes or with episodes of a duration of just seconds.\n",
    "\n",
    "From now on, all the programming will take place in this Jupyter Notebook, using PyMongo to connect to the podcasts MongoDB database.\n",
    "\n",
    "## 4.1. Connect to the database\n",
    "First, we connect to the MongoDB database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "odgKOn5tqEru"
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pprint import pprint\n",
    "\n",
    "# Connect to the MongoDB\n",
    "client = pymongo.MongoClient('localhost', 27017, username='mongoadmin', password='pass1234')\n",
    "\n",
    "# Get already created database\n",
    "database = client['finalproject']\n",
    "\n",
    "# Get podcasts collections\n",
    "podcasts = database.podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "vjbgPz1sqR8J",
    "outputId": "0eebfde2-7557-4009-d780-31f3187243e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263202\n"
     ]
    }
   ],
   "source": [
    "# Show total number of podcasts in the database\n",
    "total_num_podcasts = podcasts.count_documents({})\n",
    "print(total_num_podcasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to leave the original database intact, I will copy the ```podcasts``` collection and make all the changes on this duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263202\n"
     ]
    }
   ],
   "source": [
    "# Copy \"podcasts\" to \"podcasts_preprocessed\"\n",
    "pipeline = [ {\"$match\": {}}, \n",
    "             {\"$out\": \"podcasts_preprocessed\"},\n",
    "           ]\n",
    "\n",
    "podcasts.aggregate(pipeline)\n",
    "\n",
    "# Check size of new collection\n",
    "podcasts_prep = database.podcasts_preprocessed\n",
    "total_num_podcasts_prep = podcasts_prep.count_documents({})\n",
    "print(total_num_podcasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHeE2FGFqk_l"
   },
   "source": [
    "## 4.2. Missing values\n",
    "Missing values shall be specified as ```null``` in MongoDB. In pymongo these ```null``` values are interpreted as ```None``` so in our case, to find missing values using Python we will query for ```None``` values.\n",
    "\n",
    "As mentioned in section 3, some cases of ```null``` values appear when a podcast has no episodes. For those episodes where the number of ```total_episodes``` is 0, the ```average_duration_min```, ```release_date``` and ```last_date``` have ```null``` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 129 shows (0.00049%) with 0 episodes.\n",
      "\n",
      "Example:\n",
      "{'_id': ObjectId('61b276d16015f8bce7c4ea04'),\n",
      " 'average_duration_min': None,\n",
      " 'last_date': {'day': None, 'month': None, 'year': None},\n",
      " 'name': 'KAIJUDEPU!',\n",
      " 'release_date': {'day': None, 'month': None, 'year': None}}\n"
     ]
    }
   ],
   "source": [
    "# Show None values of show with 0 episodes\n",
    "num_shows_0_ep = podcasts_prep.count_documents({'total_episodes': 0})\n",
    "res = podcasts_prep.find({'total_episodes': 0}, {'name': 1, 'release_date' : 1, 'last_date': 1, 'average_duration_min' : 1})\n",
    "\n",
    "example = list(res)[0]\n",
    "num_shows_0ep = len(list(res))\n",
    "\n",
    "print(f'There are {num_shows_0_ep} shows ({num_shows_0_ep/total_num_podcasts_prep:.5f}%) with 0 episodes.\\n\\nExample:')\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of shows with a number of zero episodes is neglectable compared to the total number of shows that the database contains. However, since they don't give important information for our analysis, they can be removed from the database so as to have cleaner data from which to continue the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 documents deleted\n"
     ]
    }
   ],
   "source": [
    "# Remove shows with 0 episodes\n",
    "res = podcasts_prep.delete_many({'total_episodes': 0})\n",
    "print(f'{res.deleted_count} documents deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Outliers\n",
    "\n",
    "Appart from removing those podcasts which don't have any episodes, it is also important to analayze if there are any outliers that shall be removed.\n",
    "\n",
    "Let's start by looking at the duration of the episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "# Grouped by occupation\n",
    "pipeline = [\n",
    "        {'$match': {'$and': [\n",
    "                            {'education.num': {'$gte': 10}},\n",
    "                            {'work.occupation': {'$ne': 'Tech_support'}},\n",
    "                            {'work.occupation': {'$ne': 'Sales'}},\n",
    "                            {'work.occupation': {'$ne': 'Exec_managerial'}},\n",
    "                            {'work.occupation': {'$ne': 'Prof_specialty'}},\n",
    "                            {'work.occupation': {'$ne': 'Adm_clerical'}},\n",
    "                            {'work.occupation': {'$ne': None}}\n",
    "                            ]}},\n",
    "        { \"$group\": {\n",
    "            \"_id\": {\"occupation\": \"$work.occupation\"},\n",
    "            \"count\": { \"$sum\": 1 }\n",
    "        }},\n",
    "    \n",
    "        {\"$sort\": SON([(\"count\", pymongo.DESCENDING)])}    \n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MaximumValue': 690.43, 'MinimumValue': 0.0, '_id': None}\n"
     ]
    }
   ],
   "source": [
    "# TODO: greater than 0\n",
    "\n",
    "c = podcasts_prep.aggregate([\n",
    "   { \"$group\": {\n",
    "      \"_id\": None,\n",
    "      \"MaximumValue\": { \"$max\": \"$average_duration_min\" },\n",
    "      \"MinimumValue\": { \"$min\": \"$average_duration_min\" }\n",
    "   }}\n",
    "])\n",
    "\n",
    "\n",
    "pprint(list(c)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 podcasts with an average episode duration lower than 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Podcasts with an average duration of less than 5 seconds\n",
    "n_lt_5s = podcasts_prep.count_documents({'average_duration_min': {'$lt': (5/60)}})\n",
    "\n",
    "print(f'There are {n_lt_5s} podcasts with an average episode duration lower than 5 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1454 (0.0055%) podcasts with an average episode duration lower than 10 seconds\n"
     ]
    }
   ],
   "source": [
    "# Podcasts with an average duration of less than 10 seconds\n",
    "n_lt_15s = podcasts_prep.count_documents({'average_duration_min': {'$lt': (10/60)}})\n",
    "\n",
    "print(f'There are {n_lt_15s} ({n_lt_15s/total_num_podcasts_prep:.4f}%) podcasts with an average episode duration lower than 10 seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Although compared to the total number of podcasts the number of podcasts with a low average duration is not very high, it is still remarkable. Let's now look at how many of these podcasts have more than one or two episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47 (0.03%) podcasts with an average episode duration lower than 10 seconds that have more than one episode\n",
      "There are 11 (0.01%) podcasts with an average episode duration lower than 10 seconds that have more than two episodes\n"
     ]
    }
   ],
   "source": [
    "# AND total episodes more than...\n",
    "n_lt15_gt1 = podcasts_prep.count_documents({'$and': [\n",
    "                            {'average_duration_min': {'$lt': (10/60)}},\n",
    "                            {'total_episodes': {'$gt': 1}}\n",
    "                            ]})\n",
    "\n",
    "n_lt15_gt2 = podcasts_prep.count_documents({'$and': [\n",
    "                            {'average_duration_min': {'$lt': (10/60)}},\n",
    "                            {'total_episodes': {'$gt': 2}}\n",
    "                            ]})\n",
    "\n",
    "print(f'There are {n_lt15_gt1} ({n_lt15_gt1/n_lt_15s:.2f}%) podcasts with an average episode duration lower than 10 seconds that have more than one episode')\n",
    "print(f'There are {n_lt15_gt2} ({n_lt15_gt2/n_lt_15s:.2f}%) podcasts with an average episode duration lower than 10 seconds that have more than two episodes')\n",
    "# n_lt60_gt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the podcasts with an average epidose duration lower than 10 seconds, only a small fraction have more than one or two episodes.\n",
    "\n",
    "We can conclude that the shows with a short average episode duration and only one episode are outliers that will not provide insightful information for our analysis. Therefore, we can also remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407 documents deleted\n"
     ]
    }
   ],
   "source": [
    "# Remove shows with average duration lower than 10s and with less than 2 episodes\n",
    "res = podcasts_prep.delete_many({'$and': [\n",
    "                            {'average_duration_min': {'$lt': (10/60)}},\n",
    "                            {'total_episodes': {'$lt': 2}}\n",
    "                            ]})\n",
    "\n",
    "print(f'{res.deleted_count} documents deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MaximumValue': 690.43, 'MinimumValue': 0.03, '_id': None}\n"
     ]
    }
   ],
   "source": [
    "c = podcasts_prep.aggregate([\n",
    "   { \"$group\": {\n",
    "      \"_id\": None,\n",
    "      \"MaximumValue\": { \"$max\": \"$average_duration_min\" },\n",
    "      \"MinimumValue\": { \"$min\": \"$average_duration_min\" }\n",
    "   }}\n",
    "])\n",
    "\n",
    "pprint(list(c)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('61b32c432da9c43368fa2efc'),\n",
      " 'average_duration_min': 0.03,\n",
      " 'date_precision': 'day',\n",
      " 'description': 'Jooooooooo',\n",
      " 'explicit': False,\n",
      " 'id': '0gwHWLQksGwRF2r6nR6wb9',\n",
      " 'languages': ['es'],\n",
      " 'last_date': {'day': 3, 'month': 4, 'year': 2020},\n",
      " 'media_type': 'audio',\n",
      " 'name': 'Jsmoooooooo',\n",
      " 'publisher': 'Zenna',\n",
      " 'release_date': {'day': 3, 'month': 4, 'year': 2020},\n",
      " 'total_episodes': 2}\n"
     ]
    }
   ],
   "source": [
    "c = podcasts_prep.find({'$and': [\n",
    "                            {'average_duration_min': 0.03},\n",
    "                            {'total_episodes': {'$lt': 20}}\n",
    "                            ]})\n",
    "\n",
    "pprint(c.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUn6qnVkjgtW"
   },
   "source": [
    "##Â 5. Data analysis\n",
    "After cleaning the database and removing those shows that do not give relevant information, that is shows with no episodes or with only one episode and an average duration of just a few seconds, we can proceed with the data analaysis.\n",
    "\n",
    "Matplotlib and Seaborn are used for the data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Outliers\n",
    "\n",
    "An "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Exploratory Data Analysis\n",
    "\n",
    "(means, medians, quantiles, histograms, boxplots\n",
    "â¢ You should always look at every variable - you will learn something!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DeleteResult' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e8d4d5c071d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DeleteResult' object is not iterable"
     ]
    }
   ],
   "source": [
    "list(res)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "# Try to \"visualize\" each of the variables\n",
    "# Compare languages and comment\n",
    "# Episode frequency: total days / total episodes\n",
    "# Episode release dates -> compare most popular days, months\n",
    "# Year increase of podcasts\n",
    "#\n",
    "# Plus: genres?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Kg_4HJLjiiZ"
   },
   "source": [
    "##Â 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr7AYPtoiT2a"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] https://finance.yahoo.com/news/spotifys-podcast-subscriptions-expand-global-173408899.html?guccounter=1\n",
    "\n",
    "\n",
    "[2] https://www.sec.gov/Archives/edgar/data/1639920/000119312521308632/d174858dex991.htm\n",
    "\n",
    "[3] https://www.podchaser.com/articles/inside-podchaser/podchaser-raises-4m\n",
    "\n",
    "[4] https://developer.spotify.com/documentation/web-api\n",
    "\n",
    "[5] https://developer.spotify.com/community/news/2020/03/20/introducing-podcasts-api/\n",
    "\n",
    "[6] https://podcasts.apple.com/es/genre/podcasts/id26\n",
    "\n",
    "[7] https://developer.apple.com/support/enrollment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import sqlalchemy \n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(\"podcastindex_feeds.db\")    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "#Now in order to read in pandas dataframe we need to know table name\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(f\"Table Name : {cursor.fetchall()}\")\n",
    "\n",
    "df = pd.read_sql_query('SELECT * FROM podcasts', conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()   # language\n",
    "\n",
    "rslt_df = df[df['language']=='ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rslt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df = df[df['language']=='ca']\n",
    "print(len(rslt_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df = df[df['language']=='es']\n",
    "print(len(rslt_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPNJssqSlsPH"
   },
   "source": [
    "10.048.969 catalan speakers\n",
    "https://www.plataforma-llengua.cat/media/upload/pdf/informecat2018_1528713023.pdf\n",
    "\n",
    "A total of 580 million people speak Spanish in the world, 7.6% of the worldâs population.\n",
    "https://blogs.cervantes.es/londres/2019/10/15/spanish-a-language-spoken-by-580-million-people-and-only-483-million-of-them-native/\n",
    "\n",
    "Spanish 2nd number of native spakers\n",
    "https://www.babbel.com/en/magazine/the-10-most-spoken-languages-in-the-world\n",
    "\n",
    "Spotify now offers podcasts\n",
    "https://www.theverge.com/2015/5/20/8629335/spotify-adds-podcasts-videoshttps://www.theverge.com/2015/5/20/8629335/spotify-adds-podcasts-videos\n",
    "\n",
    "[Test](#1.-Introduction)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BDA-Final_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
